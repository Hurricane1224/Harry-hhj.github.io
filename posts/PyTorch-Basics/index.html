<!DOCTYPE html><html lang="zh" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="PyTorch Basics" /><meta name="author" content="Harry-hhj" /><meta property="og:locale" content="zh" /><meta name="description" content="PyTorch - Basics" /><meta property="og:description" content="PyTorch - Basics" /><link rel="canonical" href="https://harry-hhj.github.io/posts/PyTorch-Basics/" /><meta property="og:url" content="https://harry-hhj.github.io/posts/PyTorch-Basics/" /><meta property="og:site_name" content="Harry’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-08-29T11:12:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="PyTorch Basics" /><meta name="twitter:site" content="@None" /><meta name="twitter:creator" content="@Harry-hhj" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Harry-hhj"},"description":"PyTorch - Basics","url":"https://harry-hhj.github.io/posts/PyTorch-Basics/","@type":"BlogPosting","headline":"PyTorch Basics","dateModified":"2021-10-20T01:14:17+08:00","datePublished":"2021-08-29T11:12:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://harry-hhj.github.io/posts/PyTorch-Basics/"},"@context":"https://schema.org"}</script><title>PyTorch Basics | Harry's Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Harry's Blog"><meta name="application-name" content="Harry's Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script> var _hmt = _hmt || []; (function() { var hm = document.createElement("script"); hm.src = "https://hm.baidu.com/hm.js?f40851b91841f1abe810a63f8d41c2e2"; var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s); })(); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/avatar/avatar.jpeg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Harry's Blog</a></div><div class="site-subtitle font-italic">Write blogs, share ideas, make friends and enjoy life.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/Harry-hhj" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['Harry_hhj','163.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>PyTorch Basics</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>PyTorch Basics</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Harry-hhj </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Aug 29, 2021, 11:12 AM +0800" >Aug 29<i class="unloaded">2021-08-29T11:12:00+08:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Wed, Oct 20, 2021, 1:14 AM +0800" >Oct 20<i class="unloaded">2021-10-20T01:14:17+08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3394 words">18 min read</span></div></div><div class="post-content"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 480 230'%3E%3C/svg%3E" data-proofer-ignore data-src="https://raw.githubusercontent.com/Harry-hhj/Harry-hhj.github.io/master/_posts/2021-08-29-PyTorch-Basics.assets/pytorch.jpeg" class="preview-img" alt="Preview Image" width="480" height="230"><h1 id="pytorch---basics">PyTorch - Basics</h1><h2 id="pytorch-是什么">PyTorch 是什么</h2><p><strong>Torch</strong> 是一个有大量机器学习算法支持的科学计算框架，是一个与 Numpy 类似的张量 Tensor 操作库。</p><p><strong>PyTorch</strong> 是一个基于 Torch 的 Python 开源机器学习库，用于自然语言处理等应用程序。</p><p>优点：</p><ul><li>作为 Numpy 的替代品，可以使用 <strong>GPU</strong> 的强大计算能力<li>提供最大的<strong>灵活性</strong>和高速的深度学习研究平台</ul><p>缺点：</p><ul><li>全面性：目前 PyTorch 还不支持快速傅里叶、沿维翻转张量和检查无穷与非数值张量<li>性能：针对移动端、嵌入式部署以及高性能服务器端的部署其性能表现有待提升<li>文档：社区还没有那么强大，其 C 库大多数没有文档</ul><h2 id="环境搭建">环境搭建</h2><p>Miniconda3 + PyTorch</p><p>首先去 Miniconda <a href="https://docs.conda.io/en/latest/miniconda.html">官网</a>下载对应系统和 Python 版本的安装包，打开终端运行脚本，按照指令完成 conda 环境搭建。然后前往 PyTorch <a href="https://pytorch.org">官网</a>，按照需求选择，其中 Language 选择 <code class="language-plaintext highlighter-rouge">Python</code> ，Compute Platform 根据自己的硬件选择，有 Nvidia GPU 的选择 CUDA 版，有 AMD GPU 的选择 ROC 版（还需另外安装 ROC 环境），不需要或者没有 GPU 的选择 CPU 版。注意 MacOS 系统只能安装 CPU 版。复制命令并在命令行执行即可安装。</p><h2 id="预备知识">预备知识</h2><h3 id="tensor">Tensor</h3><p>Tensors （张量），与 Numpy 中的 ndarrays 类似，但是在 PyTorch 中 Tensors 可以使用 GPU 进行计算。</p><p>在讨论其语法之前，先来说说什么是张量。</p><ol><li>标量：一个单独的数<li>向量：一列有序排列的数，通过次序中的索引可以确定一个数<li>矩阵：二维数组，每个元素被两个索引唯一确定<li>张量：几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，标量是零阶张量，矢量是一阶张量，矩阵是二阶张量</ol><p>举个例子，对于任意一张彩色照片，可以表示成一个三阶张量，三个维度分别是图片的高度、宽度和 RGB 通道。下图是一个白色图片的示例：</p><p><img data-proofer-ignore data-src="https://raw.githubusercontent.com/Harry-hhj/Harry-hhj.github.io/master/_posts/2021-08-29-PyTorch-Basics.assets/640.jpeg" alt="img" /></p><p>我们继续将这一例子拓展：即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及 RGB 通道。这种数据表示形式在计算机视觉中非常常见，你可以在这里先有个印象。</p><p><strong>张量</strong>在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。</p><p>常用操作：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="c1"># 创建一个未初始化的 Tensor
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># 创建一个随机初始化的 Tensor
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># torch.rand(*sizes, out=None)-&gt;Tensor: [0, 1) 均匀分布
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># torch.randn(*sizes, out=None)-&gt;Tensor: 标准正态分布（均值为 0 ，方差为 1 ，即高斯白噪声）
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># torch.randint(low = 0, high, size, out=None, dtype=None)-&gt;Tensor: 整数范围 [low, high)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randperm</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># torch.randperm(n, out=None, dtpe=torch.int64)-&gt;LongTensor: 1 到 n 这些数的一个随机序列
</span>
<span class="c1"># 创建 Tensor 并使用现有数据初始化
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">([</span><span class="mf">5.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># 其他特殊的创建 Tensor 的方法
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>  <span class="c1"># 全 0 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>  <span class="c1"># 全 1 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 对角线为 1 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># torch.arange(s, e, step)-&gt;Tensor: 从 s 到 e ，步长为 step 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># torch.linspace(s, e, step)-&gt;Tensor: 从 s 到 e ，均匀切分成 steps 份
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># torch.normal(mean:float, std:float, size:tuple)-&gt;Tensor: 均值为 mean ，方差为 std ，大小为 size 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 均匀分布 [from, to)
</span>
<span class="c1"># new_* 方法来创建对象
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>

<span class="c1"># 根据现有的张量创建张量，重用输入张量的属性，例如 dtype ，除非设置新的值进行覆盖
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>    <span class="c1"># 覆盖 dtype ，但 size 相同
</span>
<span class="c1"># 获取 size
</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># torch.Size 返回值是 tuple 类型, 所以它支持 tuple 类型的所有操作
</span></pre></table></code></div></div><h3 id="运算">运算</h3><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre><td class="rouge-code"><pre><span class="c1"># 加法
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># Method 1
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="c1"># Method 2
</span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># Method 3
</span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">z</span><span class="p">)</span>  <span class="c1"># 提供输出tensor作为参数
# Method 4
</span><span class="n">y</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 会改变原变量的值
</span>
<span class="c1"># 索引
</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># 与 Numpy 索引方式相同
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">))</span>  <span class="c1"># .index_select(dim:int, index:Tensor(int32/64))-&gt;Tensor: 从 dim 维选取 index 的数据
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">x</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># .masked_select(mask)-&gt;Tensor: 选取掩膜为 1 处的元素，不保留原始位置信息
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">nonzero</span><span class="p">()</span>  <span class="c1"># 返回非零元素的下标
</span>
<span class="c1"># torch.gather(input, dim, index:torch.long, out=None)-&gt;Tensor：根据index，在dim维度上选取数据
# out[i][j][k]...[i+dim]...[z] = input[i][j][k]...[index[i][j][k]...[z]]_{i+dim}...[z]
</span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]))</span>	<span class="c1"># [[1 1] [4 3]]
</span>																							<span class="c1"># index 中元素范围 [0, n_dim-1]，用来指定第 dim 维的选取的位置
# 关于 torch.gather 的更多用法请参考教程：https://zhuanlan.zhihu.com/p/352877584
</span>
<span class="c1"># 改变张量的维度和大小
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>  <span class="c1"># x 和 y 共享数据
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1">#  size：-1 从其他维度推断
</span>
<span class="c1"># 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">squeeze</span><span class="p">()</span>
<span class="n">unsqueeze</span><span class="p">()</span>
</pre></table></code></div></div><p>注意：</p><ul><li>任何以 <code class="language-plaintext highlighter-rouge">_</code> 结尾的操作都会用结果<strong>替换原变量</strong>。例如：<code class="language-plaintext highlighter-rouge">x.copy_(y)</code> ， <code class="language-plaintext highlighter-rouge">x.t_()</code> ，都会改变 <code class="language-plaintext highlighter-rouge">x</code> 。<li>view() 返回的新 Tensor 与原 Tensor 虽然可能有不同的 size ，但是是<strong>共享 data 的</strong>（ view 仅仅是改变了对这个张量的观察角度，内部数据并未改变）。如果需要副本先使用 <code class="language-plaintext highlighter-rouge">.clone()</code> 。</ul><h3 id="python-数据类型转换">Python 数据类型转换</h3><p>如果你有只有一个元素的张量，使用 <code class="language-plaintext highlighter-rouge">.item()</code> 来得到 Python 数据类型的数值。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>
</pre></table></code></div></div><h3 id="numpy-转换">Numpy 转换</h3><p>将一个 Torch Tensor 转换为 NumPy 数组是一件轻松的事，反之亦然。</p><p>Torch Tensor 与 NumPy 数组<strong>共享底层内存地址</strong>，修改一个会导致另一个的变化。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="c1"># Torch Tensor -&gt; NumPy数组
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">a</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 此时 b 发生变化
# NumPy Array -&gt; Torch Tensor
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># 此时 a 发生变化
</span></pre></table></code></div></div><h3 id="broadcasting">Broadcasting</h3><p>当对两个形状不同的 Tensor 按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个 Tensor 形状相同后再按元素运算。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>  <span class="c1"># torch.Size([3, 2])
</span></pre></table></code></div></div><h3 id="cuda">CUDA</h3><p>使用 <code class="language-plaintext highlighter-rouge">.to</code> 方法 可以将 Tensor 移动到任何设备中。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="c1"># is_available 函数判断是否有 cuda 可以使用
# `torch.device` 将张量移动到指定的设备中
</span><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>          <span class="c1"># 一个 CUDA 设备对象
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># 直接从 GPU 创建张量
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                       <span class="c1"># 或者直接使用 `.to("cuda")` 将张量移动到 cuda 中
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">double</span><span class="p">))</span>       <span class="c1"># `.to` 也会对变量的类型做更改
</span></pre></table></code></div></div><p><br /></p><p>更多内容，请查看<a href="https://pytorch.org/docs/torch">官网教程</a>。</p><h2 id="autograd自动求导机制">Autograd：自动求导机制</h2><p>PyTorch 中所有神经网络的核心是 autograd 包，它为张量上的所有操作提供了<strong>自动求导</strong>。 它是一个在<strong>运行时定义</strong>的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。</p><p>在自动求导计算中有两个重要的类：</p><ul><li>Tensor<ul><li>如果设置 <code class="language-plaintext highlighter-rouge">.requires_grad</code> 为 <code class="language-plaintext highlighter-rouge">True</code>，那么将会追踪所有对于该张量的操作。当完成计算后通过调用 <code class="language-plaintext highlighter-rouge">.backward()</code>，自动计算所有的梯度，这个张量的所有梯度将会自动积累到 <code class="language-plaintext highlighter-rouge">.grad</code> 属性。<li>为了防止跟踪历史记录（和使用内存），可以将代码块包装在 <code class="language-plaintext highlighter-rouge">with torch.no_grad():</code> 中。 这在评估模型时特别有用，因为模型可能具有 <code class="language-plaintext highlighter-rouge">requires_grad = True</code> 的可训练参数，但是我们不需要梯度计算。</ul><li>Function<ul><li>Tensor 和 Function 互相连接并生成一个<strong>非循环图</strong>，它表示和存储了完整的计算历史。 每个张量都有一个 <code class="language-plaintext highlighter-rouge">.grad_fn</code> 属性，这个属性引用了一个创建了 Tensor 的 Function ，即该 Tensor 是不是通过某些运算得到的，若是，则 grad_fn 返回一个与这些运算相关的对象（除非这个张量是用户手动创建的，即，这个张量的 <code class="language-plaintext highlighter-rouge">grad_fn</code> 是 <code class="language-plaintext highlighter-rouge">None</code> ）。<li>如果需要计算导数，你可以在 Tensor 上调用 <code class="language-plaintext highlighter-rouge">.backward()</code> 。 如果 Tensor 是一个标量（即它包含一个元素数据）则不需要为 <code class="language-plaintext highlighter-rouge">backward()</code> 指定任何参数， 但是如果它有更多的元素，你需要指定一个 <code class="language-plaintext highlighter-rouge">gradient</code> 参数来匹配张量的形状。</ul></ul><p>注意：在其他的文章中你可能会看到说将 Tensor 包裹到 Variable 中提供自动梯度计算， Variable 这个在 0.41 版中已经被标注为过期了，现在可以直接使用 Tensor ，官方文档在<a href="https://pytorch.org/docs/stable/autograd.html#variable-deprecated">这里</a>。</p><h3 id="requires_grad">requires_grad</h3><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># x.grad_fn = None
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>  <span class="c1"># y.grad_fn = &lt;AddBackward0 object at 0x...&gt;
</span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>  <span class="c1"># z.grad_fn = &lt;MulBackward0 object at 0x...&gt;
</span><span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># out.grad_fn = &lt;MeanBackward0 object at 0x...&gt;
</span></pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">x</code> 是直接创建的，所以么有 <code class="language-plaintext highlighter-rouge">grad_fn</code> ，<code class="language-plaintext highlighter-rouge"> y</code> 作为操作的结果被创建，因此具有 <code class="language-plaintext highlighter-rouge">grad_fn</code> 。像 <code class="language-plaintext highlighter-rouge">x</code> 这样的节点被称为<strong>叶子节点</strong>，叶子节点对应的 <code class="language-plaintext highlighter-rouge">grad_fn</code> 是 <code class="language-plaintext highlighter-rouge">None</code> 。</p><p>输入的 <code class="language-plaintext highlighter-rouge">requires_grad</code> 在<strong>没有给定参数的情况下</strong>默认是 <code class="language-plaintext highlighter-rouge">False</code> ，可以通过 <code class="language-plaintext highlighter-rouge">requires_grad_()</code> 来改变张量的 <code class="language-plaintext highlighter-rouge">requires_grad</code> 属性。如果输入的 <code class="language-plaintext highlighter-rouge">requires_grad</code> 是 <code class="language-plaintext highlighter-rouge">False</code> ，那么之后所有计算结果的变量的 <code class="language-plaintext highlighter-rouge">requires_grad</code> 属性都将是 <code class="language-plaintext highlighter-rouge">False</code> ，且 <code class="language-plaintext highlighter-rouge">grad_fn</code> 为 None。</p><h3 id="backward">backward()</h3><p>在调用 <code class="language-plaintext highlighter-rouge">y.backward()</code> 时，如果 <code class="language-plaintext highlighter-rouge">y</code> 是标量，则不需要为 <code class="language-plaintext highlighter-rouge">backward()</code> 传入任何参数；否则，需要传入一个与 <code class="language-plaintext highlighter-rouge">y</code> 同形的 <code class="language-plaintext highlighter-rouge">Tensor</code> 。因为不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量。</p><p>在数学上，如果我们有向量值函数 $\vec y = f(\vec x)$，且 $\vec y$ 关于 $\vec x$ 的梯度是一个雅可比矩阵（Jacobian matrix）： \(J = \begin{pmatrix} \frac{\partial y_1}{\partial x_1}&amp;\cdots&amp;\frac{\partial y_1}{\partial x_n}\\ \vdots&amp;\ddots&amp;\vdots\\ \frac{\partial y_m}{\partial x_1}&amp;\cdots&amp;\frac{\partial y_m}{\partial x_n}\\ \end{pmatrix}\) 一般来说，<code class="language-plaintext highlighter-rouge">torch.autograd</code> 就是用来计算 vector-Jacobian product 的工具。也就是说，给定任一向量 $\vec v = (v_1\ v_2\ \cdots\ v_m)^T$ ，计算 $v^T \cdot J$ 。如果 $v$ 恰好是标量函数 $l = g(\vec y)$ 的梯度，也就是说 $v = (\frac{\partial l}{y_1}\ \cdots \frac{\partial l}{y_m})^T$ ，那么根据链式法则，vector-Jacobian product 是 $l$ 关于 $\vec x$ 的梯度： \(J^T \cdot v = \begin{pmatrix} \frac{\partial y_1}{\partial x_1}&amp;\cdots&amp;\frac{\partial y_1}{\partial x_n}\\ \vdots&amp;\ddots&amp;\vdots\\ \frac{\partial y_m}{\partial x_1}&amp;\cdots&amp;\frac{\partial y_m}{\partial x_n}\\ \end{pmatrix} \begin{pmatrix} \frac{\partial l}{\partial x_1}\\ \vdots\\ \frac{\partial l}{\partial x_n}\\ \end{pmatrix}\) （注意，$v^T \cdot J$ 给出了一个行向量，可以通过 $J^T \cdot v$ 将其视为列向量）</p><p>vector-Jacobian product 这种特性使得<strong>将外部梯度返回到具有非标量输出的模型</strong>变得非常方便。</p><p>以下是两个例子：</p><ul><li><p>标量求导</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1">#  因为 out 是一个纯量（scalar），out.backward() 等于 out.backward(torch.tensor(1))
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># tensor([[4.5000, 4.5000], [4.5000, 4.5000]])
</span></pre></table></code></div></div><li><p>非标量求导</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="k">while</span> <span class="n">y</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">1000</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">gradients</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</pre></table></code></div></div><p>在这个情形中，<code class="language-plaintext highlighter-rouge">y</code> 不再是个标量， <code class="language-plaintext highlighter-rouge">torch.autograd</code> 无法直接计算出完整的雅可比矩阵，但是如果我们只想要 vector-Jacobian product ，只需将向量作为参数传入 <code class="language-plaintext highlighter-rouge">backward</code> 。</p></ul><h3 id="中断梯度追踪">中断梯度追踪</h3><p><code class="language-plaintext highlighter-rouge">with torch.no_grad():</code> 中的变量将不进入梯度计算。</p><p>直接举个例子说明：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y1</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="n">y2</span><span class="p">,</span> <span class="n">y2</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">y1</span> <span class="o">+</span> <span class="n">y2</span>
<span class="k">print</span><span class="p">(</span><span class="n">y3</span><span class="p">,</span> <span class="n">y3</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">y3</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># tensor(2.)
</span></pre></table></code></div></div><p>此时 y3 的梯度经由 y1 （与 y2 无关）传播给 x ，因此 x 的梯度是 2 而不是 4 。</p><h3 id="tensordata">tensor.data</h3><p>此外，如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录（即不会影响反向传播），那么我么可以对 <strong>tensor.data</strong> 进行操作。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># 依然是一个 tensor
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>  <span class="c1"># False，即独立于计算图之外
</span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">x</span><span class="p">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mi">100</span>  <span class="c1"># 只改变了值，不会记录在计算图，所以不会影响梯度传播
</span><span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</pre></table></code></div></div><p>如果对 x 本身直接操作，将导致 x 叶子节点身份的丢失。</p><p><br /></p><p>更多内容，请查看<a href="https://pytorch.org/docs/autograd">官网教程</a>。</p><p><br /></p><p><strong>如果觉得本教程不错或对您有用，请前往项目地址 <a href="https://github.com/Harry-hhj/Harry-hhj.github.io">https://github.com/Harry-hhj/Harry-hhj.github.io</a> 点击 Star :) ，这将是对我的肯定和鼓励，谢谢！</strong></p><p><br /></p><h2 id="参考文档">参考文档</h2><ol><li><a href="https://www.jianshu.com/p/abe7515c6c7f">标量，向量，矩阵与张量</a><li><a href="https://blog.csdn.net/leilei7407/article/details/107710852">torch.rand()、torch.randn()、torch.randint()、torch.randperm()用法</a><li><a href="https://zhuanlan.zhihu.com/p/110289027">我对torch中的gather函数的一点理解</a><li><a href="https://zhuanlan.zhihu.com/p/97234180">pytorch简介和准备知识</a></ol><hr /><p>作者：Harry-hhj，github主页：<a href="https://github.com/Harry-hhj">传送门</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/tutorial/'>Tutorial</a>, <a href='/categories/pytorch/'>PyTorch</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/getting-started/" class="post-tag no-text-decoration" >getting started</a> <a href="/tags/computer-science/" class="post-tag no-text-decoration" >computer science</a> <a href="/tags/pytorch/" class="post-tag no-text-decoration" >pytorch</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=PyTorch Basics - Harry's Blog&url=https://harry-hhj.github.io/posts/PyTorch-Basics/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=PyTorch Basics - Harry's Blog&u=https://harry-hhj.github.io/posts/PyTorch-Basics/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=PyTorch Basics - Harry's Blog&url=https://harry-hhj.github.io/posts/PyTorch-Basics/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/RM-Tutorial-5-Monocular-Vision/">RM 教程 5 —— 单目视觉</a><li><a href="/posts/RM-Tutorial-Catalogue/">RoboMaster 课程目录</a><li><a href="/posts/NN-Tutorial-Catalogue/">神经网络课程目录</a><li><a href="/posts/RM-Tutorial-3-Getting-Started-with-OpenCV/">RM 教程 3 —— OpenCV 传统视觉</a><li><a href="/posts/RM-Tutorial-4-Camera/">RM 教程 4 —— 相机</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/getting-started/">getting started</a> <a class="post-tag" href="/tags/install/">install</a> <a class="post-tag" href="/tags/robomaster/">robomaster</a> <a class="post-tag" href="/tags/computer-science/">computer science</a> <a class="post-tag" href="/tags/tools/">tools</a> <a class="post-tag" href="/tags/catalog/">catalog</a> <a class="post-tag" href="/tags/opencv/">opencv</a> <a class="post-tag" href="/tags/pytorch/">pytorch</a> <a class="post-tag" href="/tags/c-c/">c/c++</a> <a class="post-tag" href="/tags/camera/">camera</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Pytorch-Building-Neural-Network/"><div class="card-body"> <span class="timeago small" >Sep 4<i class="unloaded">2021-09-04T08:00:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Pytorch Building Nueral Network</h3><div class="text-muted small"><p> Pytorch 搭建神经网络 一、热身 torch.nn 包依赖 autograd 包来定义模型并求导。 一个 nn.Module 包含： 各个层 一个 forward(input) 方法，该方法返回网络的 output 在模型中必须要定义 forward() 函数， backward() 函数（用来计算梯度）会被 autograd 自动创建。 可以在 forward()...</p></div></div></a></div><div class="card"> <a href="/posts/Deconvolution/"><div class="card-body"> <span class="timeago small" >Sep 6<i class="unloaded">2021-09-06T12:00:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deconvolution</h3><div class="text-muted small"><p> Deconvolution 一、概念 逆卷积（Deconvolution）一般和转置卷积（transposed conv）、微步卷积（fractionally strided conv）的叫法等价。其常见的用处包括： 在 ZF-Net 中用于对 feature map 做可视化 在 FCN 中用于生成等于原图 shape 的图像 无监督的 autoencoder 和 de...</p></div></div></a></div><div class="card"> <a href="/posts/Pytorch-Network-Parameter-Statistics/"><div class="card-body"> <span class="timeago small" >Sep 20<i class="unloaded">2021-09-20T18:40:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Pytorch Network Parameter Statistics</h3><div class="text-muted small"><p> PyTorch 统计网络参数量 神经网络的参数统计是很重要的，它反映了一个网络的硬件需求与性能。PyTorch 可以使用第三方库 torchsummary 来统计参数并打印层结构。但是想要正确统计出参数量，需要对如何统计参数有一定的了解。 Case1 无参数共享（最常见） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Install-dual-OS_Win10/" class="btn btn-outline-primary" prompt="Older"><p>Install dual-OS - Win10</p></a> <a href="/posts/Install-OpenCV/" class="btn btn-outline-primary" prompt="Newer"><p>Install OpenCV</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://github.com/Harry-hhj">HHJ</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/getting-started/">getting started</a> <a class="post-tag" href="/tags/install/">install</a> <a class="post-tag" href="/tags/robomaster/">robomaster</a> <a class="post-tag" href="/tags/computer-science/">computer science</a> <a class="post-tag" href="/tags/tools/">tools</a> <a class="post-tag" href="/tags/catalog/">catalog</a> <a class="post-tag" href="/tags/opencv/">opencv</a> <a class="post-tag" href="/tags/pytorch/">pytorch</a> <a class="post-tag" href="/tags/c-c/">c/c++</a> <a class="post-tag" href="/tags/camera/">camera</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
